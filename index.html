<!DOCTYPE HTML>
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-155128387-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-155128387-1');
  </script>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Tianrui Liu</title>
  <meta name="author" content="Tianrui Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <div class="container">
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Tianrui Liu</name>
              </p>
              <p style="text-align:justify">I am now a Lecturer at the Department of Computer Science, National University of Defense Technology (NUDT). 
                Before, I was a Postdoc Resesarcher in the BioMediIA @Imperial College London (advised by <a href="http://wp.doc.ic.ac.uk/dr/">Professor Daniel Rueckert</a> and <a href="https://www.imperial.ac.uk/people/b.kainz">Dr. Bernhard Kainz</a>.
                I'm interested in computer vision, deep learning, object detection and pattern recognition, image and video processing and medical image analysis.
              </p>
              <p style="text-align:justify">
                In my spare time, I like playing piano, reading and playing badminton.
              </p>
              <p style="text-align:center">
                <a href="mailto:trliu@connect.hku.hk">Email</a> &nbsp/&nbsp
                <a href="data/Tianrui_CV_2021.pdf">CV</a> &nbsp/&nbsp
               
                <a href="https://scholar.google.com/citations?user=SC53gxAAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://uk.linkedin.com/in/tianrui-liu-7b126b67"> LinkedIn </a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/tianrui_web.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/tianrui_web.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Education</heading>
              <p>
                <ul>
                <li>
                  <div>Ph.D in Computer Vision and Image Processing, Imperial College London</div>
                  <div style="float: right; text-align: right; margin: -17px; padding-right: 19px"><em>Nov. 2015 - Nov. 2019</em></div>
                </li>
                <li style="line-height:200%">
                  <div>M.Phil. in Image and Video Processing, University of Hong Kong</div>
                  <div style="float: right; text-align: right; margin: -28px; padding-right: 30px"><em>Sep. 2013 - Nov. 2015</em></div>
                </li>
                <li>
                  <div>B.Eng. in Electronic Information Engineering, Hong Kong Polytechnic University</div>
                  <div style="float: right; text-align: right; margin: -17px; padding-right: 19px"><em>Sep. 2011 - Jun. 2013</em></div>
                </li>
                <li style="line-height:200%">
                  <div>B.Eng. in Microelectronics, San Yat-Sen University</div> 
                  <div style="float: right; text-align: right; margin: -28px; padding-right: 30px"><em>Sep. 2009 - Jun. 2011</em></div>
                </li> 
              </ul>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Working Experience / Activities</heading>
              <p>
                <ul>
                <li>
                  <div>Resesarch Associate (advised by <a href="http://wp.doc.ic.ac.uk/dr/">Professor Daniel Rueckert</a> and <a href="https://www.imperial.ac.uk/people/b.kainz">Dr. Bernhard Kainz </a>.</p></div>
                  <div style="float: right; text-align: right; margin: -17px; padding-right: 19px"><em>Aug. 2019 - Aug. 2021</em></div>
                </li>
                <li>
                  <div>Research Intern (advised by <a href="https://whluo.github.io/">Dr. Wenhan Luo</a>) in Tencent AI Lab / Robotics X (Shenzhen, China)</div>
                  <div style="float: right; text-align: right; margin: -17px; padding-right: 19px"><em>Mar. 2020 - Now</em></div>
                </li>
                <li style="line-height: 200%">
                  <div>Research Intern in RISA Sicherheitsanalysen GmbH (Berlin, Germany)</div>
                  <div style="float: right; text-align: right; margin: -28px; padding-right: 30px"><em>June. 2018 - Sep. 2018</em></div>
                </li>
                <li>
                  <div>Research Intern the Hong Kong Polytechnic University, advised by <a href="https://www.eie.polyu.edu.hk/~enkmlam/">Prof. Kenneth K. M. Lam</a></div>
                  <div style="float: right; text-align: right; margin: -17px; padding-right: 19px"><em>Aug. 2018</em></div>
                </li>
                <li>
                  <div>Research Intern the Hong Kong Polytechnic University, advised by <a href="http://eie.polyu.edu.hk/~wcsiu/">Prof. Wan-chi Siu</a></div>
                  <div style="float: right; text-align: right; margin: -17px; padding-right: 19px"><em>Aug. 2018</em></div>
                </li>
                  
              </ul>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Selected Publications</heading> 
            </td>
          </tr>
        </tbody></table>
        <p>
        For a complete list, please check my <a href="https://scholar.google.com/citations?user=SC53gxAAAAAJ&hl=en">Google Scholar</a>.
        </p>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- # TIP paper: Coupled-Network for Robust Pedestrian Detection with Gated Multi-Layer Feature Extraction and Deformable Occlusion Handling -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/Ped_couple.png' height="100%" width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1912.08324.pdf">
                <papertitle>Coupled-Network for Robust Pedestrian Detection with Gated Multi-Layer Feature Extraction and Deformable Occlusion Handling</papertitle>
              </a>
              <br>
              <strong>Tianrui Liu<sup>*</sup></strong>,
              Wenhan Luo, 
              Lin Ma, 
              Junjie Huang, 
              Tania Stathaki,
              Tianhong Dai,
              <br>
              <em>arxiv Preprint</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/1912.08324.pdf">pdf</a> /
              <a href="data/interp.bib">bibtex</a>
              <br>
              <p></p>
              <p style="text-align:justify">In this paper, we propose a gated multi-layer convolutional feature extraction method which can adaptively generate discriminative features for candidate pedestrian regions.</p>
            </td>
          </tr>

          
          <!-- # paper: MICCAI: Ultrasound Video Summarization using Deep Reinforcement Learning -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/MICCAI2020_icon.png' height="100%" width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2005.09531.pdf">
                <papertitle>Ultrasound Video Summarization using Deep Reinforcement Learning</papertitle>
              </a>
              <br>
              <strong>Tianrui Liu</strong>,
              Qingjie Meng, 
              Athanasios Vlontzos, 
              Jeremy Tan, 
              Daniel Rueckert, 
              Bernhard Kainz
              <br>
              <em>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/2005.09531.pdf">pdf</a> /
              <a href="data/liir.bib">bibtex</a>
              <br>
              <p></p>
              <p style="text-align:justify">We propose an ultrasound video summarization method to summarize the long examination videos. The proposed method can remove parts that are not relevant for diagnostics and meanwhile guarantees the preservation of decisive diagnostic information.</p>
            </td>
          </tr>

          <!-- # paper: Gated Multi-layer Convolutional Feature Extraction Network for Robust Pedestrian Detection -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/gated_detection.png' height="100%" width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1910.11761.pdf">
                <papertitle>Gated Multi-layer Convolutional Feature Extraction Network for Robust Pedestrian Detection</papertitle>
              </a>
              <br>
              <strong>Tianrui Liu</strong>,
              Jun-Jie Huang,
              Tianhong Dai,
              Guangyu Ren,
              Tania Stathaki
              <br>
              <em>International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/1910.11761.pdf">pdf</a> /
              <a href="data/gated_detection.bib">bibtex</a>
              <br>
              <p></p>
              <p style="text-align:justify"> In this paper, we propose a gated multi-layer convolutional feature extraction method which can adaptively generate discriminative features for candidate pedestrian regions.</p>
            </td>
          </tr>

          <!-- # BMVC paper: SAM-RCNN: Scale-Aware Multi-Resolution Multi-Channel Pedestrian Detection -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/max_entropy.png' height="100%" width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.semanticscholar.org/paper/SAM-RCNN%3A-Scale-Aware-Multi-Resolution-Pedestrian-Liu-Elmikaty/955aa3e7317e236e41f05ec2853b64236c252af0">
                <papertitle>SAM-RCNN: Scale-Aware Multi-Resolution Multi-Channel Pedestrian Detection</papertitle>
              </a>
              <br>
              <strong>Tianrui Liu</strong>,
              Mohamed ElMikaty, 
              Tania Stathaki
              <br>
              <em>British Machine Vision Conference (BMVC)</em>, 2019
              <br>
              <a href="https://arxiv.org/pdf/1808.02246.pdf">pdf</a> /
              <a href="data/max_ent_tracker.bib">bibtex</a>
              <br>
              <p></p>
              <p style="text-align:justify">We exploits different combination of multi-resolution CNN features for pedestrian candidates of different scales.</p>
            </td>
          </tr>

          <!-- # paper: SRHRF+: Self-Example Enhanced Single Image Super-Resolution Using Hierarchical Random Forests -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/CVPRW.png' height="100%" width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/8014878">
                <papertitle>SRHRF+: Self-Example Enhanced Single Image Super-Resolution Using Hierarchical Random Forests</papertitle>
              </a>
              <br>
              Jun-jie Huang,
              <strong>Tianrui Liu</strong>,
              Pier Luigi Dragotti,
              Tania Stathaki,
              <br>
              <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop</em>, 2017
              <br>
              <a href="https://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/papers/Huang_SRHRF_Self-Example_Enhanced_CVPR_2017_paper.pdf">pdf</a> /
              <a href="data/image_synthesis.bib">bibtex</a>
              <br>
              <p></p>
              <p style="text-align:justify">We introduce CapsPix2Pix, which combines convolutional capsules with the pix2pix framework, to synthesise images conditioned on class segmentation labels. We apply our approach to a new biomedical dataset of cortical axons imaged by two-photon microscopy, as a method of data augmentation for small datasets.</p>
            </td>
          </tr>

          <!-- # paper: Faster R-CNN for robust pedestrian detection using semantic segmentation network -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/Ped_Semantic.png' height="100%" width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.frontiersin.org/articles/10.3389/fnbot.2018.00064/full">
                <papertitle>Faster R-CNN for robust pedestrian detection using semantic segmentation network</papertitle>
              </a>
              <br>
              <strong>Tianrui Liu</strong>,

              <br>
              <em>Frontiers in Neurorobotics</em>, 2018
              <br>
              <a href="https://www.frontiersin.org/articles/10.3389/fnbot.2018.00064/full">pdf</a> /
              <a href="data/axon_tracking.bib">bibtex</a>
              <br>
              <p></p>
              <p style="text-align:justify">We formulate tracking as a reinforcement learning problem, and apply deep reinforcement learning techniques with a continuous action space to learn how to track at the subpixel level.</p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Selected Awards</heading>

              <p>
                <ul>
                <li>
                  <div>Imperial College Department Scholarship (Â£130,000), Imperial College London</div>
                  <div style="float: right; text-align: right; margin: -17px; padding-right: 19px"><em>Oct. 2017 - Apr. 2022 (Expected)</em></div>
                </li>
                <li style="line-height:200%">
                  <div>Dean's List of Outstanding Students, HKPolyU</div>
                  <div style="float: right; text-align: right; margin: -28px; padding-right: 30px"><em>Oct. 2015 - Sep. 2016</em></div>
                </li>
                <li>
                  <div>Outstanding Performance Scholarship (HK$ 80,000), HKSAR Government</div>
                  <div style="float: right; text-align: right; margin: -17px; padding-right: 19px"><em>Sep. 2013 - Jun. 2015</em></div>
                </li>
                <li style="line-height:200%">
                  <div>PolyU EIE (Non-local Student) Scholarship (HK$ 100,000), HKPolyU</div> 
                  <div style="float: right; text-align: right; margin: -28px; padding-right: 30px"><em>Sep. 2011 - Jun. 2013</em></div>
                </li> 
              </ul>
              </p>
            </td>
          </tr>
        </tbody></table>



       
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <br>
                <hr>
                The original template of this page can be found from <a href="https://github.com/jonbarron/website">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>
</div>
</body>
</html>
