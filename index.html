<!DOCTYPE HTML>
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-155128387-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-155128387-1');
  </script>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Tianrui Liu</title>
  <meta name="author" content="Tianrui Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <div class="container">
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Tianrui Liu</name>
              </p>
              <p style="text-align:justify">I am now a Lecturer at the Department of Computer Science, National University of Defense Technology (NUDT). 
                I'm interested in computer vision, deep learning, object detection and pattern recognition, image and video processing and medical image analysis.
              </p>
              <p style="text-align:justify">
                In my spare time, I like playing piano, reading and playing badminton.
              </p>
              <p style="text-align:center">
                <a href="mailto:trliu@connect.hku.hk">Email</a> &nbsp/&nbsp
                <a href="data/Tianrui_CV_2021.pdf">CV</a> &nbsp/&nbsp
               
                <a href="https://scholar.google.com/citations?user=SC53gxAAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://uk.linkedin.com/in/tianrui-liu-7b126b67"> LinkedIn </a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/tianrui_web.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/tianrui_web.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Education</heading>
              <p>
                <ul>
                <li>
                  <div>PhD in Computer Vision and Image Processing, Imperial College London</div>
                  <div style="float: right; text-align: right; margin: -17px; padding-right: 19px"><em>Nov. 2015 - Nov. 2019</em></div>
                </li>
                <li style="line-height:200%">
                  <div>M.Phil in Image and Video Processing, University of Hong Kong</div>
                  <div style="float: right; text-align: right; margin: -28px; padding-right: 30px"><em>Sep. 2013 - Nov. 2015</em></div>
                </li>
                <li>
                  <div>B.Eng in Electronic Information Engineering, Hong Kong Polytechnic University</div>
                  <div style="float: right; text-align: right; margin: -17px; padding-right: 19px"><em>Sep. 2011 - Jun. 2013</em></div>
                </li>
                <li style="line-height:200%">
                  <div>BEng in Microelectronics, San Yat-Sen University</div> 
                  <div style="float: right; text-align: right; margin: -28px; padding-right: 30px"><em>Sep. 2009 - Jun. 2011</em></div>
                </li> 
              </ul>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Working Experience / Activities</heading>
              <p>
                <ul>
                <li>
                  <div>Postdoc Resesarcher (advised by <a href="http://wp.doc.ic.ac.uk/dr/">Professor Daniel Rueckert</a> and <a href="https://www.imperial.ac.uk/people/b.kainz">Dr. Bernhard Kainz.</p></div>
                  <div style="float: right; text-align: right; margin: -17px; padding-right: 19px"><em>Mar. 2020 - Now</em></div>
                </li>
                <li>
                  <div>Research Intern (advised by <a href="https://whluo.github.io/">Dr. Wenhan Luo</a>) in Tencent AI Lab / Robotics X (Shenzhen, China)</div>
                  <div style="float: right; text-align: right; margin: -17px; padding-right: 19px"><em>Mar. 2020 - Now</em></div>
                </li>
                <li style="line-height: 200%">
                  <div>Research Intern in RISA Sicherheitsanalysen GmbH (Berlin, Germany)</div>
                  <div style="float: right; text-align: right; margin: -28px; padding-right: 30px"><em>June. 2018 - Sep. 2018</em></div>
                </li>
                <li>
                  <div>Research Intern the Hong Kong Polytechnic University, advised by <a href="https://www.eie.polyu.edu.hk/~enkmlam/">Prof. Kenneth K. M. Lam</a></div>
                  <div style="float: right; text-align: right; margin: -17px; padding-right: 19px"><em>Aug. 2018</em></div>
                </li>
                <li>
                  <div>Research Intern the Hong Kong Polytechnic University, advised by <a href="http://eie.polyu.edu.hk/~wcsiu/">Prof. Wan-chi Siu</a></div>
                  <div style="float: right; text-align: right; margin: -17px; padding-right: 19px"><em>Aug. 2018</em></div>
                </li>
                  
              </ul>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Publications</heading> (<sup>*</sup> Equal Contributions)
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- # TIP paper: Coupled-Network for Robust Pedestrian Detection with Gated Multi-Layer Feature Extraction and Deformable Occlusion Handling -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/interp.png' height="100%" width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1912.08324.pdf">
                <papertitle>Coupled-Network for Robust Pedestrian Detection with Gated Multi-Layer Feature Extraction and Deformable Occlusion Handling</papertitle>
              </a>
              <br>
              <strong>Tianrui Liu<sup>*</sup></strong>,
              Wenhan Luo, 
              Lin Ma, 
              Junjie Huang, 
              Tania Stathaki,
              Tianhong Dai,
              <br>
              <em>arxiv Preprint</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/1912.08324.pdf">pdf</a> /
              <a href="data/interp.bib">bibtex</a>
              <br>
              <p></p>
              <p style="text-align:justify">In this paper, we propose a gated multi-layer convolutional feature extraction method which can adaptively generate discriminative features for candidate pedestrian regions.</p>
            </td>
          </tr>

          <!-- # paper: Gated Multi-layer Convolutional Feature Extraction Network for Robust Pedestrian Detection -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/gated_detection.png' height="100%" width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1910.11761.pdf">
                <papertitle>Gated Multi-layer Convolutional Feature Extraction Network for Robust Pedestrian Detection</papertitle>
              </a>
              <br>
              Tianrui Liu,
              Jun-Jie Huang,
              <strong>Tianhong Dai</strong>,
              Guangyu Ren,
              Tania Stathaki
              <br>
              <em>International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/1910.11761.pdf">pdf</a> /
              <a href="data/gated_detection.bib">bibtex</a>
              <br>
              <p></p>
              <p style="text-align:justify"> In this paper, we propose a gated multi-layer convolutional feature extraction method which can adaptively generate discriminative features for candidate pedestrian regions.</p>
            </td>
          </tr>

          <!-- # paper: MICCAI: Ultrasound Video Summarization using Deep Reinforcement Learning -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/liir.png' height="100%" width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://papers.nips.cc/paper/8691-liir-learning-individual-intrinsic-reward-in-multi-agent-reinforcement-learning.pdf">
                <papertitle>LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning</papertitle>
              </a>
              <br>
              Yali Du<sup>*</sup>,
              Lei Han<sup>*</sup>,
              Meng Fang,
              <strong>Tianhong Dai</strong>,
              Ji Liu,
              Dacheng Tao
              <br>
              <em>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</em>, 2020
              <br>
              <a href="http://papers.nips.cc/paper/8691-liir-learning-individual-intrinsic-reward-in-multi-agent-reinforcement-learning.pdf">pdf</a> /
              <a href="data/liir.bib">bibtex</a>
              <br>
              <p></p>
              <p style="text-align:justify">We have proposed a novel reinforcement learning based video summarzation method for ultrasound scanning videos.</p>
            </td>
          </tr>

          <!-- # paper: A Maximum Entropy Deep ReinforcementLearning Neural Tracker -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/max_entropy.png' height="100%" width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-32692-0_46">
                <papertitle>A Maximum Entropy Deep ReinforcementLearning Neural Tracker</papertitle>
              </a>
              <br>
              Shafa Balaram,
              Kai Arulkumaran,
              <strong>Tianhong Dai</strong>,
              Anil Anthony Bharath
              <br>
              <em>International Workshop on Machine Learning in Medical Imaging (MLMI)</em>, 2019
              <br>
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-32692-0_46">pdf</a> /
              <a href="data/max_ent_tracker.bib">bibtex</a>
              <br>
              <p></p>
              <p style="text-align:justify">We introducea maximum entropy continuous-action DRL neural tracker capable oftraining from scratch in a complex environment in the presence of highnoise levels, Gaussian blurring and cell detractors.</p>
            </td>
          </tr>

          <!-- # paper: Image Synthesis with a Convolutional Capsule Generative Adversarial Network -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/image_synthesis.png' height="100%" width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://proceedings.mlr.press/v102/bass19a/bass19a.pdf">
                <papertitle>Image Synthesis with a Convolutional Capsule Generative Adversarial Network</papertitle>
              </a>
              <br>
              Cher Bass,
              <strong>Tianhong Dai</strong>,
              Benjamin Billot,
              Kai Arulkumaran,
              Antonia Creswell,
              Claudia Clopath,
              Vincenzo De Paola,
              Anil Anthony Bharath
              <br>
              <em>International Conference on Medical Imaging with Deep Learning (MIDL)</em>, 2019
              <br>
              <a href="http://proceedings.mlr.press/v102/bass19a/bass19a.pdf">pdf</a> /
              <a href="data/image_synthesis.bib">bibtex</a>
              <br>
              <p></p>
              <p style="text-align:justify">We introduce CapsPix2Pix, which combines convolutional capsules with the pix2pix framework, to synthesise images conditioned on class segmentation labels. We apply our approach to a new biomedical dataset of cortical axons imaged by two-photon microscopy, as a method of data augmentation for small datasets.</p>
            </td>
          </tr>

          <!-- # paper: Deep Reinforcement Learning for Subpixel Neural Tracking -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/axon_tracking.png' height="100%" width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://proceedings.mlr.press/v102/dai19a/dai19a.pdf">
                <papertitle>Deep Reinforcement Learning for Subpixel Neural Tracking</papertitle>
              </a>
              <br>
              <strong>Tianhong Dai</strong>,
              Magda Dubois,
              Kai Arulkumaran,
              Jonathan Campbell,
              Cher Bass,
              Benjamin Billot,
              Fatmatulzehra Uslu,
              Vincenzo de Paola,
              Claudia Clopath,
              Anil Anthony Bharath
              <br>
              <em>International Conference on Medical Imaging with Deep Learning (MIDL)</em>, 2019
              <br>
              <a href="http://proceedings.mlr.press/v102/dai19a/dai19a.pdf">pdf</a> /
              <a href="data/axon_tracking.bib">bibtex</a>
              <br>
              <p></p>
              <p style="text-align:justify">We formulate tracking as a reinforcement learning problem, and apply deep reinforcement learning techniques with a continuous action space to learn how to track at the subpixel level.</p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Open Source</heading>
            </td>
          </tr>
        </tbody></table>

       
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <br>
                <hr>
                The original template of this page can be found from <a href="https://github.com/jonbarron/website">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>
</div>
</body>
</html>
